{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b649278e",
   "metadata": {},
   "source": [
    "#follow this to install prismnet\n",
    "https://github.com/kuixu/PrismNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100ca6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting logomaker\n",
      "  Using cached logomaker-0.8-py2.py3-none-any.whl (11.8 MB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from logomaker) (1.3.4)\n",
      "Requirement already satisfied: numpy in /home/hsher/.local/lib/python3.9/site-packages (from logomaker) (1.22.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from logomaker) (3.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->logomaker) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->logomaker) (6.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->logomaker) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->logomaker) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->logomaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->logomaker) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->logomaker) (1.15.0)\n",
      "Installing collected packages: logomaker\n",
      "Successfully installed logomaker-0.8\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyBigWig\n",
      "  Using cached pyBigWig-0.3.22-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (217 kB)\n",
      "Installing collected packages: pyBigWig\n",
      "Successfully installed pyBigWig-0.3.22\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install logomaker\n",
    "!pip install pyBigWig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e17a279",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prismnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_178/389116208.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mprismnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprismnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_print\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_saliency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_saliency_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_high_attention_region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#compute_high_attention_region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prismnet'"
     ]
    }
   ],
   "source": [
    "import argparse, os, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "import prismnet.model as arch\n",
    "from prismnet import train, validate, inference, log_print, compute_saliency, compute_saliency_img, compute_high_attention_region\n",
    "#compute_high_attention_region\n",
    "\n",
    "# from prismnet.engine.train_loop import \n",
    "from prismnet.model.utils import GradualWarmupScheduler\n",
    "from prismnet.loader import SeqicSHAPE\n",
    "from prismnet.utils import datautils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "outstem = 'K562_rep6'\n",
    "rbp = 'RBFOX2'\n",
    "motif = 'GCATG'\n",
    "megaoutput = pd.read_csv(f'../data/ABC_data/{outstem}.megaoutputs_masked.tsv', sep = '\\t')\n",
    "seq = pd.read_csv(f'../data/ABC_data/tsv/{outstem}.DDX3.tsv', sep = '\\t', names = ['chrom', 'name', \n",
    "                                                                                 'seq', 'struct', 'label', 'start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791872bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa595cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=seq['seq'].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab7509",
   "metadata": {},
   "source": [
    "# Evaluate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42590f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training curve\n",
    "curve = pd.read_csv(f'../data/ABC_data/{outstem}.training_curve.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve.set_index('epoch').groupby(by = 'type')['loss'].plot(legend = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f0a8b",
   "metadata": {},
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ced47",
   "metadata": {},
   "outputs": [],
   "source": [
    "megaoutput[f'logLR:{outstem}.{rbp}'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b419915",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = megaoutput.loc[megaoutput[f'logLR:{outstem}.{rbp}']>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive['logLR_bin'] = pd.cut(positive[f'logLR:{outstem}.{rbp}'], bins = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce37e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive['logLR_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only plot specific outputs\n",
    "sample_size = 64\n",
    "subset = []\n",
    "# select specific logLR as outputs\n",
    "#subset = megaoutput.loc[megaoutput[f'{outstem}.{rbp}']].sample(sample_size)\n",
    "for name,group in positive.groupby(by=['logLR_bin']):\n",
    "    try:\n",
    "        subset.append(group.sample(sample_size))\n",
    "    except:\n",
    "        subset.append(group)\n",
    "subset = pd.concat(subset, axis = 0)           \n",
    "subset[f'logLR:{outstem}.{rbp}'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0696d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prismnet.utils import datautils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c1c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df = seq.set_index('name').loc[subset['name'].tolist(), ['seq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ed170",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651132bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = datautils.convert_one_hot(subset_seq_df['seq'].tolist(), max_length)\n",
    "one_hot.shape # N, 4, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c692bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets: predict binary\n",
    "target_col = subset.columns[(subset.columns.str.startswith('logLR'))&(subset.columns.str.contains(outstem))]\n",
    "target_df = subset[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89910e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = target_df.values\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train, test = datautils.split_dataset(data, targets, valid_frac=0.2)\n",
    "\n",
    "target_data_type = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b10c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prismnet.model.PrismNet import *\n",
    "class PrismNet_Multitask(nn.Module):\n",
    "    def __init__(self, mode=\"pu\", output_dim=10):\n",
    "        super(PrismNet_Multitask, self).__init__()\n",
    "        self.mode = mode\n",
    "        h_p, h_k = 2, 5 \n",
    "        if mode==\"pu\":\n",
    "            self.n_features = 5\n",
    "        elif mode==\"seq\":\n",
    "            self.n_features = 4\n",
    "            h_p, h_k = 1, 3 \n",
    "        elif mode==\"str\":\n",
    "            self.n_features = 1\n",
    "            h_p, h_k = 0, 1\n",
    "        else:\n",
    "            raise \"mode error\"\n",
    "        \n",
    "        base_channel = 8\n",
    "        self.conv    = Conv2d(1, base_channel, kernel_size=(11, h_k), bn = True, same_padding=True)\n",
    "        self.se      = SEBlock(base_channel)\n",
    "        self.res2d   = ResidualBlock2D(base_channel, kernel_size=(11, h_k), padding=(5, h_p)) \n",
    "        self.res1d   = ResidualBlock1D(base_channel*4) \n",
    "        self.avgpool = nn.AvgPool2d((1,self.n_features))\n",
    "        self.gpool   = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc      = nn.Linear(base_channel*4*8, output_dim)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"[forward]\n",
    "        \n",
    "        Args:\n",
    "            input ([tensor],N,C,W,H): input features\n",
    "        \"\"\"\n",
    "        if self.mode==\"seq\":\n",
    "            input = input[:,:,:,:4]\n",
    "        elif self.mode==\"str\":\n",
    "            input = input[:,:,:,4:]\n",
    "        x = self.conv(input)\n",
    "        x = F.dropout(x, 0.1, training=self.training)\n",
    "        z = self.se(x)\n",
    "        x = self.res2d(x*z)\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2])\n",
    "        x = self.res1d(x)\n",
    "        x = F.dropout(x, 0.3, training=self.training)\n",
    "        x = self.gpool(x)\n",
    "        x = x.view(x.shape[0], x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrismNet_Multitask(mode = 'seq', output_dim = targets.shape[1])\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'../data/ABC_data/{outstem}.model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqicSHAPE_Multitask(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, is_infer=False, use_structure=True):\n",
    "        \"\"\"data loader\n",
    "        \n",
    "        Args:\n",
    "            data_path ([str]): h5 file path\n",
    "            is_test (bool, optional): testset or not. Defaults to False.\n",
    "        \"\"\"\n",
    "        if is_infer:\n",
    "            self.dataset = self.__load_infer_data__(data_path, use_structure=use_structure)\n",
    "            print(\"infer data: \", self.__len__(),\" use_structure: \", use_structure)\n",
    "        else:\n",
    "#             dataset = h5py.File(data_path, 'r')\n",
    "#             X_train = np.array(dataset['X_train']).astype(np.float32)\n",
    "#             Y_train = np.array(dataset['Y_train']).astype(np.int32)\n",
    "#             X_test  = np.array(dataset['X_test']).astype(np.float32)\n",
    "#             Y_test  = np.array(dataset['Y_test']).astype(np.int32)\n",
    "            X = np.array(X).astype(np.float32)\n",
    "            Y = np.array(Y).astype(np.float32)\n",
    "#             if len(Y_train.shape) == 1:\n",
    "#                 Y_train = np.expand_dims(Y_train, axis=1)\n",
    "#                 Y_test  = np.expand_dims(Y_test, axis=1)\n",
    "            X = np.expand_dims(X, axis=3).transpose([0, 3, 2, 1]) # N, 1, length, channel\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "#             labels, nums = np.unique(Y_train,return_counts=True)\n",
    "#             print(\"train:\", labels, nums)\n",
    "#             labels, nums = np.unique(Y_test,return_counts=True)\n",
    "#             print(\"test:\", labels, nums)\n",
    "\n",
    "#             train = self.__prepare_data__(train)\n",
    "#             test  = self.__prepare_data__(test)\n",
    "\n",
    "            \n",
    "            self.dataset = {'inputs': X, 'targets': Y}\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    def __load_infer_data__(self, data_path, use_structure=True):\n",
    "        from prismnet.utils import datautils\n",
    "        dataset = datautils.load_testset_txt(data_path, use_structure=use_structure, seq_length=101)\n",
    "        return dataset\n",
    "       \n",
    "    \n",
    "    def __prepare_data__(self, data):\n",
    "        inputs    = data['inputs'][:,:,:,:4]\n",
    "        structure = data['inputs'][:,:,:,4:]\n",
    "        structure = np.expand_dims(structure[:,:,:,0], axis=3)\n",
    "        inputs    = np.concatenate([inputs, structure], axis=3)\n",
    "        data['inputs']  = inputs\n",
    "        return data\n",
    "\n",
    "    def __to_sequence__(self, x):\n",
    "        x1 = np.zeros_like(x[0,:,:1])\n",
    "        for i in range(x1.shape[0]):\n",
    "            # import pdb; pdb.set_trace()\n",
    "            x1[i] = np.argmax(x[0,i,:4])\n",
    "            # import pdb; pdb.set_trace()\n",
    "        return x1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        x = self.dataset['inputs'][index]\n",
    "        # x = self.__to_sequence__(x)\n",
    "        y = self.dataset['targets'][index]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['inputs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "    \n",
    "    \n",
    "#     SeqicSHAPE_Multitask(train[0], train[1], is_infer=False, use_structure=False), \n",
    "# batch_size=64, shuffle=True,  **kwargs)\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(\n",
    "    SeqicSHAPE_Multitask(data, targets, is_infer=False, use_structure=False),\n",
    "batch_size=64*8, shuffle=False, **kwargs)\n",
    "\n",
    "#print(\"Train set:\", len(train_loader.dataset)) #X_train (example=91099, ATCGshape=5, length=101)\n",
    "#X_train (example=91099, ATCGshape=5, length=101)\n",
    "#Y_train (example=91099, binary_outcome=1))\n",
    "print(\"Test  set:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer):\n",
    "    ''' train for one epoch'''\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (x0, y0) in enumerate(train_loader):\n",
    "        x, y = x0.float().to(device), y0.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        epoch_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss\n",
    "def validate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    y_all = []\n",
    "    p_all = []\n",
    "    l_all = []\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x0, y0) in enumerate(test_loader):\n",
    "            x, y = x0.float().to(device), y0.to(device).float()\n",
    "            \n",
    "            \n",
    "            output  = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            y_np = output.to(device='cpu', dtype=torch.float32).numpy()\n",
    "            y_all.append(y_np)\n",
    "            \n",
    "\n",
    "    return epoch_loss, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04293561",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-6)\n",
    "nepoch = 60\n",
    "scheduler = GradualWarmupScheduler(\n",
    "    optimizer, multiplier=8, total_epoch=nepoch, after_scheduler=None)\n",
    "criterion = torch.nn.MSELoss()\n",
    "batch_size= 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, y_test_pred = validate(model, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred=np.concatenate(y_test_pred)\n",
    "for rbp_idx, name in zip(range(test[1].shape[1]), target_df.columns):\n",
    "    plt.scatter(test[1][:64, rbp_idx], y_pred[:64, rbp_idx])\n",
    "    plt.ylabel('Y_pred')\n",
    "    plt.xlabel('test')\n",
    "    print(pearsonr(test[1][:64, rbp_idx],y_pred[:64, rbp_idx]))\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ac1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# \n",
    "# Kui Xu, xukui.cs@gmail.com\n",
    "# 2019-02-25\n",
    "# ref smoothGrad\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad,Variable\n",
    "import numpy as np\n",
    "\n",
    "class SmoothGrad(object):\n",
    "    def __init__(self, model, device='cpu', only_seq=False, train=False, \n",
    "        x_stddev=0.015, t_stddev=0.015, nsamples=20, magnitude=2):\n",
    "        self.model     = model\n",
    "        self.device    = device\n",
    "        self.train     = train\n",
    "        self.only_seq  = only_seq\n",
    "        self.x_stddev  = x_stddev\n",
    "        self.t_stddev  = t_stddev\n",
    "        self.nsamples  = nsamples\n",
    "        self.magnitude = magnitude\n",
    "        self.features  = model\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "    def get_gradients(self, z, pred_label=None, rbp_idx = 0):\n",
    "        self.model.eval()\n",
    "        self.model.zero_grad()\n",
    "        z = z.to(self.device)\n",
    "        z.requires_grad=True\n",
    "        output = self.model(z)\n",
    "        \n",
    "        output[:,rbp_idx].backward() # now it is a multioutput.... maybe sum is not the right way tho!\n",
    "        return z.grad\n",
    "\n",
    "    def get_smooth_gradients(self, z, y=None, rbp_idx=0):\n",
    "        return self.__call__(z, y, rbp_idx = rbp_idx)\n",
    "        \n",
    "    def __call__(self, z, y=None,rbp_idx = 0):\n",
    "        \"\"\"[summary]\n",
    "        \n",
    "        Args:\n",
    "            z ([type]): [description] X\n",
    "            y ([type]): [description] Y\n",
    "            x_stddev (float, optional): [description]. Defaults to 0.15.\n",
    "            t_stddev (float, optional): [description]. Defaults to 0.15.\n",
    "            nsamples (int, optional):   [description]. Defaults to 20.\n",
    "            magnitude (int, optional):  magnitude:0,1,2; 0: original gradient, 1: absolute value of the gradient,\n",
    "                                        2: square value of the gradient. Defaults to 2.\n",
    "        \n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. for sequece\n",
    "        x = z[:,:,:,:4] # .data.cpu()\n",
    "        x_stddev   = (self.x_stddev * (x.max()-x.min())).to(self.device).item() \n",
    "\n",
    "        total_grad = torch.zeros(z.shape).to(self.device)\n",
    "        x_noise    = torch.zeros(x.shape).to(self.device)\n",
    "        if not self.only_seq:\n",
    "            # 2. for structure  \n",
    "            t = z[:,:,:,4:] #.data.cpu()\n",
    "            t_stddev = (self.t_stddev * (t.max()-t.min())).to(self.device).item() \n",
    "            #t_total_grad = torch.zeros(t.shape)\n",
    "            t_noise = torch.zeros(t.shape).to(self.device)\n",
    "\n",
    "        for i in range(self.nsamples):\n",
    "            x_plus_noise = x + x_noise.zero_().normal_(0, x_stddev)\n",
    "            if self.only_seq:\n",
    "                z_plus_noise = x_plus_noise\n",
    "            else:\n",
    "                t_plus_noise = t + t_noise.zero_().normal_(0, t_stddev)\n",
    "                z_plus_noise = torch.cat((x_plus_noise, t_plus_noise), dim=3)\n",
    "            #print(\"z_plus_noise:\",z_plus_noise.size())\n",
    "            grad = self.get_gradients(z_plus_noise, y, rbp_idx = rbp_idx)\n",
    "            if self.magnitude == 1:\n",
    "                total_grad += torch.abs(grad)\n",
    "            elif self.magnitude == 2:\n",
    "                total_grad += grad * grad\n",
    "            \n",
    "            # total_grad += grad * grad\n",
    "        total_grad /= self.nsamples\n",
    "        return total_grad\n",
    "\n",
    "    def get_batch_gradients(self, X, Y=None, rbp_idx = 0):\n",
    "        if Y is not None:\n",
    "            assert len(X) == len(Y), \"The size of input {} and target {} are not matched.\".format(len(X), len(Y))\n",
    "        g = torch.zeros_like(X)\n",
    "        for i in range(X.shape[0]):\n",
    "            x        = X[i:i+1]\n",
    "            if Y is not None:\n",
    "                y    = Y[i:i+1]\n",
    "            else:\n",
    "                y    = None\n",
    "            g[i:i+1] =  self.get_smooth_gradients(x, y, rbp_idx = rbp_idx)\n",
    "            # g[i:i+1] =  self.get_gradients(x, y)\n",
    "        return g\n",
    "\n",
    "\n",
    "def generate_saliency(model, x, y=None, smooth=False, nsamples=2, stddev=0.15, only_seq=False, \\\n",
    "    train=False):\n",
    "    saliency = SmoothGrad(model, only_seq, train)\n",
    "    x_grad   = saliency.get_smooth_gradients(x, y, nsamples=nsamples, x_stddev=stddev, t_stddev=stddev)\n",
    "    return x_grad\n",
    "\n",
    "\n",
    "\n",
    "class GuidedBackpropReLU(torch.autograd.Function):\n",
    "\n",
    "    def __init__(self, inplace=False):\n",
    "        super(GuidedBackpropReLU, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        pos_mask = (input > 0).type_as(input)\n",
    "        output = torch.addcmul(\n",
    "            torch.zeros(input.size()).type_as(input),\n",
    "            input,\n",
    "            pos_mask)\n",
    "        self.save_for_backward(input, output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, output = self.saved_tensors\n",
    "\n",
    "        pos_mask_1 = (input > 0).type_as(grad_output)\n",
    "        pos_mask_2 = (grad_output > 0).type_as(grad_output)\n",
    "        grad_input = torch.addcmul(\n",
    "            torch.zeros(input.size()).type_as(input),\n",
    "            torch.addcmul(\n",
    "                torch.zeros(input.size()).type_as(input), grad_output, pos_mask_1),\n",
    "                pos_mask_2)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + inplace_str + ')'\n",
    "\n",
    "class GuidedBackpropSmoothGrad(SmoothGrad):\n",
    "\n",
    "    def __init__(self, model, device='cpu', only_seq=False, train=False, \n",
    "        x_stddev=0.15, t_stddev=0.15, nsamples=20, magnitude=2):\n",
    "        super(GuidedBackpropSmoothGrad, self).__init__(\n",
    "            model, device, only_seq, train, x_stddev, t_stddev, nsamples, magnitude)\n",
    "        for idx, module in self.features._modules.items():\n",
    "            if module.__class__.__name__ is 'ReLU':\n",
    "                self.features._modules[idx] = GuidedBackpropReLU()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "sgrad = GuidedBackpropSmoothGrad(model, device=device, only_seq = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp_idx=np.where(target_col.str.contains(rbp))[0][0]\n",
    "gs = []\n",
    "for batch_idx, (x0, y0) in enumerate(test_loader):\n",
    "    X, Y = x0.float().to(device), y0.to(device).float()\n",
    "    output = model(X)\n",
    "    \n",
    "    guided_saliency = sgrad.get_batch_gradients(X, Y, rbp_idx = rbp_idx)\n",
    "    gs.append(guided_saliency)\n",
    "gs = torch.cat(gs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_saliency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product of sequence and guided saliency\n",
    "max_saliency = (np.transpose(np.expand_dims(one_hot, axis =1), (0,1,3,2))*gs.cpu().numpy()\n",
    "               ).sum(axis = -1).max(axis = -1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df['is_motif']=subset_seq_df['seq'].str.contains(motif)\n",
    "subset_seq_df['n_motif']=subset_seq_df['seq'].str.count(motif)\n",
    "subset_seq_df['max_saliency'] = max_saliency\n",
    "subset_seq_df[f'logLR:{outstem}.{rbp}'] = subset.set_index('name')[f'logLR:{outstem}.{rbp}']\n",
    "subset_seq_df['rbp_idx'] = np.arange(subset_seq_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ded0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df.index.tolist()==subset['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f76773",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df.boxplot(by = 'is_motif', column = ['max_saliency', f'logLR:{outstem}.{rbp}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c5c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df.loc[(subset_seq_df['is_motif'])].sort_values(by = 'n_motif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64792ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df = subset_seq_df.merge(megaoutput[['name','gene_name', 'feature_type_top', 'start']], left_index = True,\n",
    "                    right_on = 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd28ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyBigWig\n",
    "import numpy as np\n",
    "\n",
    "class strand_specific_wig:\n",
    "    def __init__(self, plus, minus):\n",
    "        self.plus = pyBigWig.open(plus)\n",
    "        self.minus = pyBigWig.open(minus)\n",
    "        \n",
    "    def fetch(self, chrom = None, start= None, end=None, strand= None, interval = None):\n",
    "        ''' return icSHAPE reacitivity for a bedtool interval or chrom, start, end, strand'''\n",
    "        if interval:\n",
    "            start = interval.start\n",
    "            end = interval.end\n",
    "            strand = interval.strand\n",
    "            chrom = interval.chrom\n",
    "        if strand == '-':\n",
    "            icshape_data = self.minus\n",
    "        else:\n",
    "            icshape_data = self.plus\n",
    "        values = icshape_data.values(chrom, start, end)\n",
    "        if strand == '-':\n",
    "            values = values[::-1]\n",
    "        return np.nan_to_num(np.array(values), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17011fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp_wig_cits = strand_specific_wig(f'../data/CITS/{rbp}.pos.bw',\n",
    "                                 f'../data/CITS/{rbp}.neg.bw'\n",
    "                                 )\n",
    "rbp_wig_cov = strand_specific_wig(f'../data/COV/{rbp}.pos.bw',\n",
    "                             f'../data/COV/{rbp}.neg.bw'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import logomaker\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def plot_saliency(index):\n",
    "    index2seq = 'ACGU'\n",
    "    \n",
    "\n",
    "    # find coverage\n",
    "    window_name = subset_seq_df.loc[subset_seq_df['rbp_idx']==index, 'name'].values[0]\n",
    "    row = megaoutput.loc[megaoutput['name']==window_name].iloc[0]\n",
    "    wig_values = rbp_wig_cov.fetch(row['chrom'], row['start'], row['end'], row['strand'])\n",
    "    wig_values_cits = rbp_wig_cits.fetch(row['chrom'], row['start'], row['end'], row['strand'])\n",
    "\n",
    "    seq_values = one_hot[index,:,:] # [1,4,100]\n",
    "    gradient_values = gs[index,0,:,:].cpu().numpy() # [1,1,100,4]\n",
    "    saliency_values = gradient_values * seq_values.T # 4*100\n",
    "    saliency_df = pd.DataFrame(saliency_values.T)\n",
    "    saliency_df.index = list(index2seq)\n",
    "\n",
    "    saliency_df = saliency_df.loc[:, seq_values.sum(axis = 0)!=0].T\n",
    "    saliency_df.index = np.arange(saliency_df.shape[0])\n",
    "\n",
    "    f, ax = plt.subplots(2,1, sharex = True, figsize = (12,4))\n",
    "    logomaker.Logo(saliency_df, # only plot places with sequence\n",
    "                              shade_below=.5,\n",
    "                              fade_below=.5,\n",
    "                              font_name='Arial Rounded MT Bold', \n",
    "                  ax = ax[0])\n",
    "\n",
    "\n",
    "    ax[0].set_ylabel('saliency')\n",
    "    ax[1].bar(np.arange(len(wig_values)), wig_values, color = 'lightgrey')\n",
    "    ax[1].set_ylabel('coverage')\n",
    "#     ax[2].bar(np.arange(len(wig_values_cits)), wig_values_cits, color = 'lightgrey')\n",
    "#     ax[2].plot(gaussian_filter1d(wig_values_cits, 3), color = 'tomato', label = 'smoothed CITS')\n",
    "#     ax[2].set_ylabel('#CITS')\n",
    "\n",
    "    ax[0].set_title(row['chrom'] + ':'+str(row['start'])+'-'+str(row['end'])+':'+row['strand'])\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a84a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_seq_df.loc[subset_seq_df['feature_type_top']=='UTR3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b00f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_saliency(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_saliency(indices):\n",
    "    index2seq = 'ACGU'\n",
    "    \n",
    "\n",
    "    # find coverage\n",
    "    all_wig_value = []\n",
    "    all_saliency_values = []\n",
    "    window_names = subset_seq_df.loc[subset_seq_df['rbp_idx'].isin(indices), 'name']\n",
    "    sub = megaoutput.loc[megaoutput['name'].isin(window_names)].iloc[0]\n",
    "    for index in indices:\n",
    "        window_name = subset_seq_df.loc[subset_seq_df['rbp_idx']==index, 'name'].values[0]\n",
    "        row = megaoutput.loc[megaoutput['name']==window_name].iloc[0]\n",
    "        wig_values = rbp_wig_cov.fetch(row['chrom'], row['start'], row['end'], row['strand'])\n",
    "        wig_values_cits = rbp_wig_cits.fetch(row['chrom'], row['start'], row['end'], row['strand'])\n",
    "\n",
    "        seq_values = one_hot[index,:,:] # [1,4,100]\n",
    "        gradient_values = gs[index,0,:,:].cpu().numpy() # [1,1,100,4]\n",
    "        saliency_values = gradient_values * seq_values.T # 4*100\n",
    "        saliency_df = pd.DataFrame(saliency_values.T)\n",
    "        saliency_df.index = list(index2seq)\n",
    "\n",
    "        saliency_df = saliency_df.loc[:, seq_values.sum(axis = 0)!=0].T\n",
    "        saliency_df.index = np.arange(saliency_df.shape[0])\n",
    "        \n",
    "        all_wig_value.append(wig_values)\n",
    "        all_saliency_values.append(saliency_df)\n",
    "        \n",
    "    \n",
    "    all_wig_value = np.concatenate(all_wig_value)\n",
    "    all_saliency_df = pd.concat(all_saliency_values, axis = 0, ignore_index = True)\n",
    "    print(all_saliency_df)\n",
    "    \n",
    "    f, ax = plt.subplots(2,1, sharex = True, figsize = (60,4))\n",
    "    logomaker.Logo(all_saliency_df, # only plot places with sequence\n",
    "                              shade_below=.5,\n",
    "                              fade_below=.5,\n",
    "                              font_name='Arial Rounded MT Bold', \n",
    "                  ax = ax[0])\n",
    "\n",
    "\n",
    "    ax[0].set_ylabel('saliency')\n",
    "    ax[1].bar(np.arange(len(all_wig_value)), all_wig_value, color = 'lightgrey')\n",
    "    ax[1].set_ylabel('coverage')\n",
    "    \n",
    "\n",
    "    ax[0].set_title(row['chrom'] + ':'+str(sub['start'].min())+'-'+str(sub['end'].max())+':'+row['strand'])\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plot_saliency([772,773,774,775])\n",
    "plt.suptitle('NORAD')\n",
    "plt.savefig('NORAD_PUM2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8b5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
